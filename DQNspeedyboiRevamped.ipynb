{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/15 [00:00<?, ?it/s]Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 58, 62, 64)        1664      \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 28, 30, 64)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 13, 14, 80)        46160     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 80)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 3, 3, 128)         41088     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 1, 1, 128)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 128)               0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               16512     \n_________________________________________________________________\nlayer1 (Dense)               (None, 32)                4128      \n_________________________________________________________________\nlayer2 (Dense)               (None, 7)                 231       \n=================================================================\nTotal params: 109,783\nTrainable params: 109,783\nNon-trainable params: 0\n_________________________________________________________________\nNone\ntoo little info\ntoo little info\ntoo little info\n  7%|▋         | 1/15 [00:05<01:16,  5.45s/it]breaking\n 13%|█▎        | 2/15 [00:08<01:01,  4.73s/it]breaking\n 20%|██        | 3/15 [00:11<00:51,  4.25s/it]breaking\n 27%|██▋       | 4/15 [00:14<00:43,  3.91s/it]breaking\n 33%|███▎      | 5/15 [00:17<00:36,  3.67s/it]breaking\n 40%|████      | 6/15 [00:21<00:31,  3.51s/it]breaking\n 47%|████▋     | 7/15 [00:24<00:27,  3.40s/it]breaking\n 53%|█████▎    | 8/15 [00:27<00:23,  3.32s/it]breaking\n 60%|██████    | 9/15 [00:30<00:19,  3.27s/it]breaking\n 67%|██████▋   | 10/15 [00:33<00:16,  3.23s/it]breaking\n 73%|███████▎  | 11/15 [00:36<00:13,  3.26s/it]breaking\n 80%|████████  | 12/15 [00:40<00:09,  3.23s/it]breaking\n 87%|████████▋ | 13/15 [00:43<00:06,  3.19s/it]breaking\n 93%|█████████▎| 14/15 [00:46<00:03,  3.17s/it]breaking\n100%|██████████| 15/15 [00:49<00:00,  3.30s/it]breaking\n\n"
    }
   ],
   "source": [
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import time\n",
    "class Agent:\n",
    "    def __init__(self, inputs, outputs):\n",
    "\n",
    "        # load json and create model\n",
    "        # json_file = open('modelwack.json', 'r')\n",
    "        # loaded_model_json = json_file.read()\n",
    "        # json_file.close()\n",
    "        # loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "        # self.model = loaded_model\n",
    "        self.model = Sequential([\n",
    "            Conv2D(64, (5,5), input_shape=(120,128,1), strides = (2,2), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(3, 3), strides = (2,2)),\n",
    "            Conv2D(80, (3,3), activation='relu', strides = (2,2)),\n",
    "            MaxPooling2D(pool_size=(3, 3), strides = (2,2)),\n",
    "            Conv2D(128, (2,2), strides = (2,2), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(3, 3), strides = (2,2)),\n",
    "            Flatten(),\n",
    "            Dense(128, input_dim=inputs,activation='relu'),\n",
    "            Dense(32, activation=\"relu\", name=\"layer1\"),\n",
    "            Dense(outputs, activation=\"linear\", name=\"layer2\"),\n",
    "        ])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "        # self.model.load_weights(\"modelwack3.h5\")\n",
    "        self.memory = []\n",
    "        print(self.model.summary())\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "        self.lastAction = 0\n",
    "        self.secondlastAction = 0\n",
    "\n",
    "    def predict(self, state):\n",
    "        stateConv = state\n",
    "        # stateConv = np.squeeze(state).reshape(1,-1)\n",
    "        qval = self.model.predict(np.reshape(stateConv, (1,120,128,1)))\n",
    "        return qval\n",
    "    def act(self, state):\n",
    "        qval = self.predict(state)\n",
    "        prob = tf.nn.softmax(tf.math.divide((qval.flatten()), 0.7))\n",
    "        # print(np.array(prob))\n",
    "        action = np.random.choice(range(7), p = np.array(prob))\n",
    "        self.secondlastAction = self.lastAction\n",
    "        self.lastAction = action\n",
    "        return action\n",
    "        \n",
    "\n",
    "    def remember(self, state, nextState, action, reward, done):\n",
    "        self.memory.append(np.array([state, nextState, action, reward, done]))\n",
    "\n",
    "    def learn(self):\n",
    "        self.batchSize = 32\n",
    "\n",
    "        if len(self.memory) > 100000:\n",
    "            self.memory = []\n",
    "            print(\"trimming memory\")\n",
    "        if len(self.memory) < self.batchSize:\n",
    "            print(\"too little info\")\n",
    "            return #still need to learn, too little memory\n",
    "        batch = random.sample(self.memory, self.batchSize)\n",
    "        #check how much time random samples take too\n",
    "        history = []\n",
    "\n",
    "        self.learnBatch(batch)\n",
    "\n",
    "    def learnBatch(self, batch, alpha = 0.9):\n",
    "        batch = np.array(batch)\n",
    "        actions = batch[:,2].reshape(self.batchSize).tolist()\n",
    "        rewards = batch[:,3].reshape(self.batchSize).tolist()\n",
    "\n",
    "        stateToPredict = batch[:,0].reshape(self.batchSize).tolist()\n",
    "        nextStateToPredict = batch[:,1].reshape(self.batchSize).tolist()\n",
    "\n",
    "        statePrediction = self.model.predict(np.reshape(stateToPredict, (self.batchSize, 120, 128,1)))\n",
    "        nextStatePrediction = self.model.predict(np.reshape(nextStateToPredict, (self.batchSize, 120, 128,1)))\n",
    "        statePrediction = np.array(statePrediction)\n",
    "        nextStatePrediction = np.array(nextStatePrediction)\n",
    "\n",
    "        for i in range(self.batchSize):\n",
    "            action = actions[i]\n",
    "            reward = rewards[i]\n",
    "            nextState = nextStatePrediction[i]\n",
    "            qval = statePrediction[i, action]\n",
    "            statePrediction[i, action] += alpha * (reward + 0.95* np.max(nextState) - qval)\n",
    "            # # doubleq^\n",
    "\n",
    "        self.xTrain.append(np.reshape(stateToPredict, (self.batchSize, 120, 128,1)))\n",
    "        self.yTrain.append(statePrediction)\n",
    "        history = self.model.fit(self.xTrain, self.yTrain, batch_size=5,epochs=1, verbose=0)\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "\n",
    "    def imagePreprocess(self, state):\n",
    "        # state[0:20] = np.zeros((20,256,3)) + (((self.secondlastAction) / 6)*255)\n",
    "        state[20:40] = np.zeros((20,256,3)) + (((self.lastAction) / 6)*255)\n",
    "        img = Image.fromarray(state)\n",
    "        img = img.resize((120,128),Image.ANTIALIAS)\n",
    "        bwImg = img.convert('LA')\n",
    "        # bwImg.show() #use to debug but otherwise this will spam your computer with images\n",
    "        bwImg = np.array(bwImg, dtype = np.float32)/255\n",
    "        toRet = np.reshape(bwImg[:,:,0], (120,128,1))\n",
    "        return toRet\n",
    "\n",
    "\n",
    "\n",
    "inputs = (240,256,3) #env.observation_space\n",
    "inputs = 5\n",
    "outputs = 7 #env.action_space \n",
    "agent = Agent(inputs, outputs)\n",
    "# print(agent.model.summary())\n",
    "plotX = []\n",
    "# for i_episode in tqdm(range(200)): #episode counter\n",
    "for i_episode in tqdm(range(15)): #episode counter\n",
    "    state = agent.imagePreprocess(env.reset()) #reset the game\n",
    "    epReward = 0\n",
    "    for t in (range(500)):\n",
    "        # start_time2= time.time()\n",
    "        # action = env.action_space.sample()\n",
    "        env.render()\n",
    "        action = agent.act(state)\n",
    "        totalR = 0\n",
    "        for _ in range(5): #speeds up learning if you combine various episodes\n",
    "            nextState, reward, done, info = env.step(action)\n",
    "            totalR += reward\n",
    "            if done == True:\n",
    "                break\n",
    "        if done == True:\n",
    "            print(\"breaking\")\n",
    "            break\n",
    "        nextState = agent.imagePreprocess(nextState)\n",
    "        agent.remember(state, nextState, action, totalR, done)\n",
    "        state = nextState\n",
    "\n",
    "        epReward += totalR\n",
    "        if totalR < -9: #consider it episode done\n",
    "            plotX.append(epReward)\n",
    "            epReward = 0\n",
    "\n",
    "        plotX.append(totalR)        \n",
    "\n",
    "        # \n",
    "        # print(\"--- %s process time seconds ---\" % (time.time() - start_time2))\n",
    "        if t % 12 == 0:\n",
    "            history = agent.learn()\n",
    "\n",
    "    # model_json = agent.model.to_json()\n",
    "    # with open(\"modelwack3.json\", \"w\") as json_file:\n",
    "    #     json_file.write(model_json)\n",
    "    # # serialize weights to HDF5\n",
    "    # agent.model.save_weights(\"modelwack3.h5\")\n",
    "    # print(\"Saved model to disk\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i_episode in tqdm(range(10)): #episode counter\n",
    "    state = agent.imagePreprocess(env.reset()) #reset the game\n",
    "    for t in tqdm(range(10000)):\n",
    "        # start_time2= time.time()\n",
    "        env.render()\n",
    "        # action = env.action_space.sample()\n",
    "        if t % 3 == 0:\n",
    "            action = agent.act(state)\n",
    "            nextState, reward, done, info = env.step(action)\n",
    "            nextState = agent.imagePreprocess(nextState)\n",
    "            agent.remember(state, nextState, action, reward, done)\n",
    "        else: \n",
    "            nextState, reward, done, _ = env.step(action)\n",
    "            nextState = agent.imagePreprocess(nextState)\n",
    "\n",
    "        state = nextState\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "        # \n",
    "        # print(\"--- %s process time seconds ---\" % (time.time() - start_time2))\n",
    "        if t % 15 == 0:\n",
    "            history = agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit48ab55c67191446ab6f17aa00e0ca628"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}