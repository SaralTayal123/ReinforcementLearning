{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(plotX)),plotX) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from mss import mss\n",
    "from PIL import Image, ImageEnhance\n",
    "import keyboard\n",
    "# import pyautogui\n",
    "import time\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf                                                               \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = Sequential([\n",
    "            Conv2D(32, (8,8), input_shape=(76, 384, 3),\n",
    "                   strides=(2,2), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(5,5), strides=(2, 2)),\n",
    "            Conv2D(64, (4,4), activation='relu', strides=(1,1)),\n",
    "            MaxPooling2D(pool_size=(7, 7), strides=(3, 3)),\n",
    "            Conv2D(128, (1, 1), strides=(1,1), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(3,3), strides=(3,3)),\n",
    "            Flatten(),\n",
    "            Dense(384, activation='relu'),\n",
    "            Dense(64, activation=\"relu\", name=\"layer1\"),\n",
    "            Dense(8, activation=\"relu\", name=\"layer2\"),\n",
    "            Dense(3, activation=\"linear\", name=\"layer3\"), #2 outputs\n",
    "        ])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "        # self.model.load_weights(\"DinoGameredux.h5\")\n",
    "        self.memory = []\n",
    "        # print(self.model.summary())\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "        self.loss = []\n",
    "\n",
    "\n",
    "    def predict(self, state):\n",
    "        stateConv = state\n",
    "        # stateConv = np.squeeze(state).reshape(1,-1)\n",
    "        qval = self.model.predict(np.reshape(stateConv, (1, 76, 384, 3)))\n",
    "        return qval\n",
    "\n",
    "    def act(self, state):\n",
    "        qval = self.predict(state)\n",
    "        # prob = tf.nn.softmax(tf.math.divide((qval.flatten()), 0.6)) #0.7 is the temperature/exploration factor\n",
    "        # print(np.array(prob))\n",
    "        z = np.random.random()\n",
    "        if z > 0.1:\n",
    "            # print(np.argmax(qval.flatten()))\n",
    "            return np.argmax(qval.flatten())\n",
    "        else:\n",
    "            return np.random.choice(range(3))\n",
    "        # action = np.random.choice(range(3), p=np.array(prob))\n",
    "        # return action\n",
    "\n",
    "    def remember(self, state, nextState, action, reward, done):\n",
    "        self.memory.append(np.array([state, nextState, action, reward, done]))\n",
    "\n",
    "    def learn(self):\n",
    "        self.batchSize = 128\n",
    "\n",
    "        if len(self.memory) > 100000:\n",
    "            self.memory = []\n",
    "            print(\"trimming memory\")\n",
    "        if len(self.memory) < self.batchSize:\n",
    "            print(\"too little info\")\n",
    "            return  # still need to learn, too little memory\n",
    "        batch = random.sample(self.memory, self.batchSize)\n",
    "        #check how much time random samples take too\n",
    "\n",
    "        self.learnBatch(batch)\n",
    "\n",
    "    def learnBatch(self, batch, alpha=0.8):\n",
    "        batch = np.array(batch)\n",
    "        actions = batch[:, 2].reshape(self.batchSize).tolist()\n",
    "        rewards = batch[:, 3].reshape(self.batchSize).tolist()\n",
    "\n",
    "        stateToPredict = batch[:, 0].reshape(self.batchSize).tolist()\n",
    "        nextStateToPredict = batch[:, 1].reshape(self.batchSize).tolist()\n",
    "\n",
    "        statePrediction = self.model.predict(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 3)))\n",
    "        nextStatePrediction = self.model.predict(np.reshape(\n",
    "            nextStateToPredict, (self.batchSize, 76, 384, 3)))\n",
    "        statePrediction = np.array(statePrediction)\n",
    "        nextStatePrediction = np.array(nextStatePrediction)\n",
    "\n",
    "        for i in range(self.batchSize):\n",
    "            action = actions[i]\n",
    "            reward = rewards[i]\n",
    "            nextState = nextStatePrediction[i]\n",
    "            qval = statePrediction[i, action]\n",
    "            if reward < -5: \n",
    "                statePrediction[i, action] = reward\n",
    "            else:\n",
    "                statePrediction[i, action] += alpha * (reward + 0.95 * np.max(nextState) - qval)\n",
    "            # # doubleq^\n",
    "\n",
    "        self.xTrain.append(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 3)))\n",
    "        self.yTrain.append(statePrediction)\n",
    "        history = self.model.fit(\n",
    "            self.xTrain, self.yTrain, batch_size=5, epochs=1, verbose=0)\n",
    "        loss = history.history.get(\"loss\")[0]\n",
    "        print(\"LOSS: \", loss)\n",
    "        self.loss.append(loss)\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Enviornment:\n",
    "    def __init__(self):\n",
    "        self.mon = {'top': 243, 'left': 0, 'width': 1366, 'height': 270}\n",
    "        # self.mon = {'top': 380, 'left': 0, 'width': 1920, 'height': 380}\n",
    "        # self.mon = {'top': 1000, 'left': 0, 'width': 3840, 'height': 760}\n",
    "        self.sct = mss()\n",
    "        self.counter = 0\n",
    "        self.startTime = -1\n",
    "        self.imageBank = []\n",
    "        self.imageBankLength = 3 #number of frames for the conv net\n",
    "        self.actionMemory = 2 #init as 2 to show no action taken   \n",
    "        #image processing\n",
    "        self.ones = np.ones((76,384,3))\n",
    "        self.zeros = np.zeros((76,384,3))  \n",
    "        self.zeros1 = np.zeros((76,384,3))\n",
    "        self.zeros2 = np.zeros((76,384,3))\n",
    "        self.zeros3 = np.zeros((76,384,3))\n",
    "        self.zeros1[:,:,0] = 1\n",
    "        self.zeros2[:,:,1] = 1\n",
    "        self.zeros3[:,:,2] = 1\n",
    "\n",
    "    def startGame(self):\n",
    "        #start the game, giving the user a few seconds to click on the chrome tab after starting the code\n",
    "        for i in reversed(range(3)):\n",
    "            print(\"game starting in \", i)\n",
    "            time.sleep(1)\n",
    "\n",
    "    def step(self, action):        \n",
    "        actions ={\n",
    "            0: 'space',\n",
    "            1: 'down'\n",
    "        }            \n",
    "        if action != self.actionMemory:\n",
    "            if self.actionMemory != 2:\n",
    "                keyboard.release(actions.get(self.actionMemory))\n",
    "            if action != 2:\n",
    "                keyboard.press(actions.get(action))\n",
    "        self.actionMemory = action\n",
    "\n",
    "        screenshot = self.sct.grab(self.mon)\n",
    "        img = np.array(screenshot)[:, :, 0]\n",
    "        processedImg = self._processImg(img)\n",
    "        state = self._imageBankHandler(processedImg)\n",
    "        done = self._done(processedImg)\n",
    "        reward = self._getReward(done)\n",
    "        return state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.startTime = time.time()\n",
    "        keyboard.press(\"space\")\n",
    "        time.sleep(0.5)\n",
    "        keyboard.release(\"space\")\n",
    "        return self.step(0)\n",
    "\n",
    "    def _processImg(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "        img = img.resize((384, 76), Image.ANTIALIAS)\n",
    "        # img = ImageEnhance.Contrast(img).enhance(5)\n",
    "        img = self._contrast(img)\n",
    "        img = np.reshape(img, (76,384))\n",
    "        return img\n",
    "\n",
    "    def _contrast(self,pixvals):\n",
    "        minval = 32 #np.percentile(pixvals, 2)\n",
    "        maxval = 171 #np.percentile(pixvals, 98)\n",
    "        pixvals = np.clip(pixvals, minval, maxval)\n",
    "        pixvals = ((pixvals - minval) / (maxval - minval))\n",
    "        # Image.fromarray(pixvals.astype(np.uint8))\n",
    "        return pixvals\n",
    "\n",
    "    def _imageBankHandler(self, img):\n",
    "        # timeTest = time.time()\n",
    "        img = np.array(img)\n",
    "        # cv2.imshow(\"image\", img)\n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        #     cv2.destroyAllWindows()\n",
    "        while len(self.imageBank) < (self.imageBankLength): \n",
    "            self.imageBank.append(np.reshape(img,(76,384,1)) * self.ones)\n",
    "\n",
    "        \n",
    "        bank = np.array(self.imageBank)\n",
    "        toReturn = self.zeros\n",
    "        img1 = (np.reshape(img,(76,384,1)) * self.ones)  * self.zeros1\n",
    "        img2 = bank[0] * self.zeros2\n",
    "        img3 = bank[1] * self.zeros3\n",
    "\n",
    "\n",
    "        toReturn = np.array(img1 + img2 + img3)\n",
    "        # toReturn = np.reshape(toReturn, (76,384,4))\n",
    "        \n",
    "\n",
    "        self.imageBank.pop(0)\n",
    "        self.imageBank.append(np.reshape(img,(76 ,384,1)) * self.ones)\n",
    "\n",
    "        # cv2.imshow(\"image\", np.reshape(toReturn[:,:,0], (76,384,1)))\n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "        #     cv2.destroyAllWindows()\n",
    "\n",
    "        # print(\"bank Time for loop: \", time.time()-timeTest)\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    def _getReward(self,done):\n",
    "        if done:\n",
    "            return -10\n",
    "        else: \n",
    "            return 1\n",
    "            return time.time() - self.startTime\n",
    "        \n",
    "    def _done(self,img):\n",
    "        img = np.array(img)\n",
    "        img  = img[30:50, 180:203]\n",
    "        # cv2.imshow(\"image\",img)\n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "        #     cv2.destroyAllWindows()\n",
    "\n",
    "        val = np.sum(img)\n",
    "        expectedVal = 331.9352517985612\n",
    "        # print(\"val: \", val)\n",
    "        # print(\"Difference: \", np.absolute(val-expectedVal))\n",
    "        if np.absolute(val-expectedVal) > 15: #seems to work well\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "game starting in  2\ngame starting in  1\ngame starting in  0\n  0%|          | 1/1500 [00:05<2:10:48,  5.24s/it]breaking\nAvg Frame-Rate:  18.777736275443004\n-10\ntoo little info\nSaved model to disk\n  0%|          | 2/1500 [00:05<1:35:44,  3.83s/it]breaking\n-10\ntoo little info\nbreaking\nAvg Frame-Rate:  18.834553658184383\n-10\n  0%|          | 3/1500 [00:12<1:57:47,  4.72s/it]LOSS:  0.7308729887008667\nbreaking\nAvg Frame-Rate:  19.338373910527903\n-10\n  0%|          | 4/1500 [00:18<2:04:45,  5.00s/it]LOSS:  0.7152766585350037\nbreaking\nAvg Frame-Rate:  18.57861824847282\n-10\n  0%|          | 5/1500 [00:24<2:10:21,  5.23s/it]LOSS:  0.5000467896461487\nbreaking\nAvg Frame-Rate:  19.401403830418605\n-10\n  0%|          | 6/1500 [00:29<2:13:49,  5.37s/it]LOSS:  0.9893577694892883\nbreaking\nAvg Frame-Rate:  19.007467541360118\n-10\n  0%|          | 7/1500 [00:36<2:25:52,  5.86s/it]LOSS:  0.5210997462272644\nbreaking\nAvg Frame-Rate:  19.40321367629818\n-10\n  1%|          | 8/1500 [00:42<2:24:37,  5.82s/it]LOSS:  0.13112519681453705\nbreaking\nAvg Frame-Rate:  18.890577262849956\n-10\n  1%|          | 9/1500 [00:49<2:30:59,  6.08s/it]LOSS:  1.0790245532989502\nbreaking\nAvg Frame-Rate:  18.909152460334447\n-10\n  1%|          | 10/1500 [00:54<2:27:41,  5.95s/it]LOSS:  0.5374882817268372\nbreaking\nAvg Frame-Rate:  19.45275571689779\n-10\n  1%|          | 11/1500 [01:00<2:25:40,  5.87s/it]LOSS:  0.949491024017334\n"
    }
   ],
   "source": [
    "plotX = []\n",
    "while True:\n",
    "# if __name__ == \"__main__\":\n",
    "    agent = Agent() #currently agent is configured with only 2 actions\n",
    "    env = Enviornment()\n",
    "    env.startGame()    \n",
    "    for i in tqdm(range(1500)):\n",
    "        state, reward, done = env.reset()\n",
    "        epReward = 0\n",
    "        done = False\n",
    "        episodeTime = time.time()\n",
    "        stepCounter = 0\n",
    "        while not done:\n",
    "            # startTime = time.time()\n",
    "            action = agent.act(state)\n",
    "            nextState, reward, done = env.step(action)\n",
    "            agent.remember(state, nextState, action, reward, done)\n",
    "            if done == True:\n",
    "                print(\"breaking\")\n",
    "                break\n",
    "            state = nextState\n",
    "            stepCounter += 1\n",
    "            # print(\"episode time: \", time.time()-startTime)\n",
    "            # print('\\n')\n",
    "\n",
    "        #post episode\n",
    "        if stepCounter != 0:\n",
    "            print(\"Avg Frame-Rate: \", 1/((time.time()-episodeTime)/stepCounter))\n",
    "        plotX.append(reward)\n",
    "        print(reward)\n",
    "        agent.learn()\n",
    "\n",
    "\n",
    "       \n",
    "        if i % 20 == 0:\n",
    "            agent .model.save_weights (\"DinoGameSpeed.h5\")\n",
    "            print( \"Saved model to disk\")                    \n",
    "# \n",
    "            # print(\"Time action prediction : \", time.time()-start_time2)\n",
    "            # start_time2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(plotX)),plotX) \n",
    "plt.show()\n",
    "plt.plot(range(len(agent.loss)),agent.loss) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittfconda2248cc4e27cb4999a15a76ba7df35744",
   "display_name": "Python 3.7.7 64-bit ('tf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}