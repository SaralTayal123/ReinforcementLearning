{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from mss import mss\n",
    "from PIL import Image, ImageEnhance\n",
    "import keyboard\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "\n",
    "class Enviornment:\n",
    "    def __init__(self):\n",
    "        self.mon = {'top': 380, 'left': 1920, 'width': 1920, 'height': 380}\n",
    "        self.sct = mss()\n",
    "        self.counter = 0\n",
    "        self.startTime = -1\n",
    "        self.imageBank = []\n",
    "        self.imageBankLength = 4 #number of frames for the conv net\n",
    "\n",
    "    def startGame(self):\n",
    "        #start the game, giving the user a few seconds to click on the chrome tab after starting the code\n",
    "        for i in reversed(range(3)):\n",
    "            print(\"game starting in \", i)\n",
    "            time.sleep(1)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            pass\n",
    "            keyboard.press_and_release('space')\n",
    "        if action == 1:\n",
    "            pass\n",
    "            keyboard.press_and_release('down')\n",
    "        if action == 2:\n",
    "            junk = 5\n",
    "\n",
    "        screenshot = self.sct.grab(self.mon)\n",
    "        img = np.array(screenshot)[:, :, 0]\n",
    "        processedImg = self._processImg(img)\n",
    "        state = self._imageBankHandler(processedImg)\n",
    "        done = self._done(processedImg)\n",
    "        reward = self._getReward(done)\n",
    "\n",
    "        return state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.startTime = time.time()\n",
    "        return self.step(0)\n",
    "\n",
    "    def _processImg(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "        img = img.resize((384, 76), Image.ANTIALIAS)\n",
    "        # img = ImageEnhance.Contrast(img).enhance(5)\n",
    "        img = self._contrast(img)\n",
    "        img = np.reshape(img, (76,384, 1))\n",
    "        return img\n",
    "\n",
    "    def _contrast(self,pixvals):\n",
    "        minval = 32 #np.percentile(pixvals, 2)\n",
    "        maxval = 171 #np.percentile(pixvals, 98)\n",
    "        pixvals = np.clip(pixvals, minval, maxval)\n",
    "        pixvals = ((pixvals - minval) / (maxval - minval))\n",
    "        # Image.fromarray(pixvals.astype(np.uint8))\n",
    "        return pixvals\n",
    "\n",
    "    def _imageBankHandler(self, img):\n",
    "        while len(self.imageBank) < (self.imageBankLength): \n",
    "            self.imageBank.append(img)\n",
    "\n",
    "        bank = [] + self.imageBank #easy way to deep copy\n",
    "        toReturn = [] + bank + img\n",
    "        toReturn = np.array(toReturn)\n",
    "        toReturn = np.reshape(toReturn, (76,384,self.imageBankLength))\n",
    "\n",
    "        #handle image saving and trimming\n",
    "        self.imageBank.pop(0)\n",
    "        self.imageBank.append(img)\n",
    "        \n",
    "        return toReturn\n",
    "\n",
    "    def _getReward(self,done):\n",
    "        if done:\n",
    "            return -50\n",
    "        else: \n",
    "            return 1\n",
    "            return time.time() - self.startTime\n",
    "        \n",
    "    def _done(self,img):\n",
    "        img = np.array(img)\n",
    "        img  = img[30:50, 180:203, :]\n",
    "        # print(np.sum(img))\n",
    "           \n",
    "        # cv2.imshow(\"image\", img)\n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        #     cv2.destroyAllWindows()\n",
    "\n",
    "        # listToCheck = [(18, 137), (19, 152), (16, 165), (16, 178),\n",
    "        #                (17, 206), (16, 216), (17, 232), (17, 247)]              \n",
    "        # val = 0\n",
    "        # for elem in listToCheck:\n",
    "        #     val += img[elem][0]\n",
    "        # val = val/8  # avg\n",
    "        # expectedVal = 0.025179856115107917\n",
    "\n",
    "        val = np.sum(img)\n",
    "        expectedVal = 331.9352517985612\n",
    "        # print(\"val: \", val)\n",
    "        # print(\"Difference: \", np.absolute(val-expectedVal))\n",
    "        if np.absolute(val-expectedVal) > 1: #seems to work well\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = Sequential([\n",
    "            Conv2D(32, (8,8), input_shape=(76, 384, 4),\n",
    "                   strides=(2,2), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(5,5), strides=(2, 2)),\n",
    "            Conv2D(64, (4,4), activation='relu', strides=(1,1)),\n",
    "            MaxPooling2D(pool_size=(7, 7), strides=(3, 3)),\n",
    "            Conv2D(128, (1, 1), strides=(1,1), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(3,3), strides=(3,3)),\n",
    "            Flatten(),\n",
    "            Dense(384, activation='relu'),\n",
    "            Dense(64, activation=\"relu\", name=\"layer1\"),\n",
    "            Dense(8, activation=\"relu\", name=\"layer2\"),\n",
    "            Dense(2, activation=\"linear\", name=\"layer3\"), #2 outputs\n",
    "        ])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "        # self.model.load_weights(\"modelwack3.h5\")\n",
    "        self.memory = []\n",
    "        print(self.model.summary())\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "\n",
    "\n",
    "    def predict(self, state):\n",
    "        stateConv = state\n",
    "        # stateConv = np.squeeze(state).reshape(1,-1)\n",
    "        qval = self.model.predict(np.reshape(stateConv, (1, 76, 384, 4)))\n",
    "        return qval\n",
    "\n",
    "    def act(self, state):\n",
    "        qval = self.predict(state)\n",
    "        prob = tf.nn.softmax(tf.math.divide((qval.flatten()), 0.7)) #0.7 is the temperature/exploration factor\n",
    "        # print(np.array(prob))\n",
    "        action = np.random.choice(range(2), p=np.array(prob))\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, nextState, action, reward, done):\n",
    "        self.memory.append(np.array([state, nextState, action, reward, done]))\n",
    "\n",
    "    def learn(self):\n",
    "        self.batchSize = 64\n",
    "\n",
    "        if len(self.memory) > 100000:\n",
    "            self.memory = []\n",
    "            print(\"trimming memory\")\n",
    "        if len(self.memory) < self.batchSize:\n",
    "            print(\"too little info\")\n",
    "            return  # still need to learn, too little memory\n",
    "        batch = random.sample(self.memory, self.batchSize)\n",
    "        #check how much time random samples take too\n",
    "\n",
    "        self.learnBatch(batch)\n",
    "\n",
    "    def learnBatch(self, batch, alpha=0.9):\n",
    "        batch = np.array(batch)\n",
    "        actions = batch[:, 2].reshape(self.batchSize).tolist()\n",
    "        rewards = batch[:, 3].reshape(self.batchSize).tolist()\n",
    "\n",
    "        stateToPredict = batch[:, 0].reshape(self.batchSize).tolist()\n",
    "        nextStateToPredict = batch[:, 1].reshape(self.batchSize).tolist()\n",
    "\n",
    "        statePrediction = self.model.predict(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        nextStatePrediction = self.model.predict(np.reshape(\n",
    "            nextStateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        statePrediction = np.array(statePrediction)\n",
    "        nextStatePrediction = np.array(nextStatePrediction)\n",
    "\n",
    "        for i in range(self.batchSize):\n",
    "            action = actions[i]\n",
    "            reward = rewards[i]\n",
    "            nextState = nextStatePrediction[i]\n",
    "            qval = statePrediction[i, action]\n",
    "            statePrediction[i, action] += alpha * (reward + 0.95 * np.max(nextState) - qval)\n",
    "            # # doubleq^\n",
    "\n",
    "        self.xTrain.append(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        self.yTrain.append(statePrediction)\n",
    "        history = self.model.fit(\n",
    "            self.xTrain, self.yTrain, batch_size=5, epochs=1, verbose=0)\n",
    "        loss = history.history.get(\"loss\")[0]\n",
    "        print(\"LOSS: \", loss)\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "\n",
    "\n",
    "plotX = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent() #currently agent is configured with only 2 actions\n",
    "    env = Enviornment()\n",
    "    env.startGame()    \n",
    "    for i in range(2):\n",
    "        state, reward, done = env.reset()\n",
    "        epReward = 0\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done = env.step(0)\n",
    "            \n",
    "            totalR = 0\n",
    "            for _ in range(4):  # speeds up learning if you skip frames\n",
    "                nextState, reward, done = env.step(2) #noop action\n",
    "                totalR += reward\n",
    "                if done == True:\n",
    "                    break\n",
    "            if done == True:\n",
    "                print(\"breaking\")\n",
    "                break\n",
    "            agent.remember(state, nextState, action, totalR, done)\n",
    "            state = nextState\n",
    "            epReward += totalR\n",
    "\n",
    "        plotX.append(epReward)\n",
    "        print(epReward)\n",
    "        agent.learn()\n",
    "        time.sleep(1)import numpy as np\n",
    "import cv2 as cv2\n",
    "from mss import mss\n",
    "from PIL import Image, ImageEnhance\n",
    "import keyboard\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "\n",
    "class Enviornment:\n",
    "    def __init__(self):\n",
    "        self.mon = {'top': 380, 'left': 1920, 'width': 1920, 'height': 380}\n",
    "        self.sct = mss()\n",
    "        self.counter = 0\n",
    "        self.startTime = -1\n",
    "        self.imageBank = []\n",
    "        self.imageBankLength = 4 #number of frames for the conv net\n",
    "\n",
    "    def startGame(self):\n",
    "        #start the game, giving the user a few seconds to click on the chrome tab after starting the code\n",
    "        for i in reversed(range(3)):\n",
    "            print(\"game starting in \", i)\n",
    "            time.sleep(1)\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            pass\n",
    "            keyboard.press_and_release('space')\n",
    "        if action == 1:\n",
    "            pass\n",
    "            keyboard.press_and_release('down')\n",
    "        if action == 2:\n",
    "            junk = 5\n",
    "\n",
    "        screenshot = self.sct.grab(self.mon)\n",
    "        img = np.array(screenshot)[:, :, 0]\n",
    "        processedImg = self._processImg(img)\n",
    "        state = self._imageBankHandler(processedImg)\n",
    "        done = self._done(processedImg)\n",
    "        reward = self._getReward(done)\n",
    "\n",
    "        return state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.startTime = time.time()\n",
    "        return self.step(0)\n",
    "\n",
    "    def _processImg(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "        img = img.resize((384, 76), Image.ANTIALIAS)\n",
    "        # img = ImageEnhance.Contrast(img).enhance(5)\n",
    "        img = self._contrast(img)\n",
    "        img = np.reshape(img, (76,384, 1))\n",
    "        return img\n",
    "\n",
    "    def _contrast(self,pixvals):\n",
    "        minval = 32 #np.percentile(pixvals, 2)\n",
    "        maxval = 171 #np.percentile(pixvals, 98)\n",
    "        pixvals = np.clip(pixvals, minval, maxval)\n",
    "        pixvals = ((pixvals - minval) / (maxval - minval))\n",
    "        # Image.fromarray(pixvals.astype(np.uint8))\n",
    "        return pixvals\n",
    "\n",
    "    def _imageBankHandler(self, img):\n",
    "        while len(self.imageBank) < (self.imageBankLength): \n",
    "            self.imageBank.append(img)\n",
    "\n",
    "        bank = [] + self.imageBank #easy way to deep copy\n",
    "        toReturn = [] + bank + img\n",
    "        toReturn = np.array(toReturn)\n",
    "        toReturn = np.reshape(toReturn, (76,384,self.imageBankLength))\n",
    "\n",
    "        #handle image saving and trimming\n",
    "        self.imageBank.pop(0)\n",
    "        self.imageBank.append(img)\n",
    "        \n",
    "        return toReturn\n",
    "\n",
    "    def _getReward(self,done):\n",
    "        if done:\n",
    "            return -50\n",
    "        else: \n",
    "            return 1\n",
    "            return time.time() - self.startTime\n",
    "        \n",
    "    def _done(self,img):\n",
    "        img = np.array(img)\n",
    "        img  = img[30:50, 180:203, :]\n",
    "        # print(np.sum(img))\n",
    "           \n",
    "        # cv2.imshow(\"image\", img)\n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        #     cv2.destroyAllWindows()\n",
    "\n",
    "        # listToCheck = [(18, 137), (19, 152), (16, 165), (16, 178),\n",
    "        #                (17, 206), (16, 216), (17, 232), (17, 247)]              \n",
    "        # val = 0\n",
    "        # for elem in listToCheck:\n",
    "        #     val += img[elem][0]\n",
    "        # val = val/8  # avg\n",
    "        # expectedVal = 0.025179856115107917\n",
    "\n",
    "        val = np.sum(img)\n",
    "        expectedVal = 331.9352517985612\n",
    "        # print(\"val: \", val)\n",
    "        # print(\"Difference: \", np.absolute(val-expectedVal))\n",
    "        if np.absolute(val-expectedVal) > 1: #seems to work well\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.model = Sequential([\n",
    "            Conv2D(32, (8,8), input_shape=(76, 384, 4),\n",
    "                   strides=(2,2), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(5,5), strides=(2, 2)),\n",
    "            Conv2D(64, (4,4), activation='relu', strides=(1,1)),\n",
    "            MaxPooling2D(pool_size=(7, 7), strides=(3, 3)),\n",
    "            Conv2D(128, (1, 1), strides=(1,1), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(3,3), strides=(3,3)),\n",
    "            Flatten(),\n",
    "            Dense(384, activation='relu'),\n",
    "            Dense(64, activation=\"relu\", name=\"layer1\"),\n",
    "            Dense(8, activation=\"relu\", name=\"layer2\"),\n",
    "            Dense(2, activation=\"linear\", name=\"layer3\"), #2 outputs\n",
    "        ])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "        # self.model.load_weights(\"modelwack3.h5\")\n",
    "        self.memory = []\n",
    "        print(self.model.summary())\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "\n",
    "\n",
    "    def predict(self, state):\n",
    "        stateConv = state\n",
    "        # stateConv = np.squeeze(state).reshape(1,-1)\n",
    "        qval = self.model.predict(np.reshape(stateConv, (1, 76, 384, 4)))\n",
    "        return qval\n",
    "\n",
    "    def act(self, state):\n",
    "        qval = self.predict(state)\n",
    "        prob = tf.nn.softmax(tf.math.divide((qval.flatten()), 0.7)) #0.7 is the temperature/exploration factor\n",
    "        # print(np.array(prob))\n",
    "        action = np.random.choice(range(2), p=np.array(prob))\n",
    "        return action\n",
    "\n",
    "    def remember(self, state, nextState, action, reward, done):\n",
    "        self.memory.append(np.array([state, nextState, action, reward, done]))\n",
    "\n",
    "    def learn(self):\n",
    "        self.batchSize = 64\n",
    "\n",
    "        if len(self.memory) > 100000:\n",
    "            self.memory = []\n",
    "            print(\"trimming memory\")\n",
    "        if len(self.memory) < self.batchSize:\n",
    "            print(\"too little info\")\n",
    "            return  # still need to learn, too little memory\n",
    "        batch = random.sample(self.memory, self.batchSize)\n",
    "        #check how much time random samples take too\n",
    "\n",
    "        self.learnBatch(batch)\n",
    "\n",
    "    def learnBatch(self, batch, alpha=0.9):\n",
    "        batch = np.array(batch)\n",
    "        actions = batch[:, 2].reshape(self.batchSize).tolist()\n",
    "        rewards = batch[:, 3].reshape(self.batchSize).tolist()\n",
    "\n",
    "        stateToPredict = batch[:, 0].reshape(self.batchSize).tolist()\n",
    "        nextStateToPredict = batch[:, 1].reshape(self.batchSize).tolist()\n",
    "\n",
    "        statePrediction = self.model.predict(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        nextStatePrediction = self.model.predict(np.reshape(\n",
    "            nextStateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        statePrediction = np.array(statePrediction)\n",
    "        nextStatePrediction = np.array(nextStatePrediction)\n",
    "\n",
    "        for i in range(self.batchSize):\n",
    "            action = actions[i]\n",
    "            reward = rewards[i]\n",
    "            nextState = nextStatePrediction[i]\n",
    "            qval = statePrediction[i, action]\n",
    "            statePrediction[i, action] += alpha * (reward + 0.95 * np.max(nextState) - qval)\n",
    "            # # doubleq^\n",
    "\n",
    "        self.xTrain.append(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        self.yTrain.append(statePrediction)\n",
    "        history = self.model.fit(\n",
    "            self.xTrain, self.yTrain, batch_size=5, epochs=1, verbose=0)\n",
    "        loss = history.history.get(\"loss\")[0]\n",
    "        print(\"LOSS: \", loss)\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "\n",
    "\n",
    "plotX = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = Agent() #currently agent is configured with only 2 actions\n",
    "    env = Enviornment()\n",
    "    env.startGame()    \n",
    "    for i in range(2):\n",
    "        state, reward, done = env.reset()\n",
    "        epReward = 0\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done = env.step(0)\n",
    "            \n",
    "            totalR = 0\n",
    "            for _ in range(4):  # speeds up learning if you skip frames\n",
    "                nextState, reward, done = env.step(2) #noop action\n",
    "                totalR += reward\n",
    "                if done == True:\n",
    "                    break\n",
    "            if done == True:\n",
    "                print(\"breaking\")\n",
    "                break\n",
    "            agent.remember(state, nextState, action, totalR, done)\n",
    "            state = nextState\n",
    "            epReward += totalR\n",
    "\n",
    "        plotX.append(epReward)\n",
    "        print(epReward)\n",
    "        agent.learn()\n",
    "        time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}